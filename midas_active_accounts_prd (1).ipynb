{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a41b2f-4b67-46ab-9095-364ba43bd7b6",
   "metadata": {},
   "source": [
    "### midas_active_accounts_prd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a59c2b-3cd6-4560-8de4-5fc9ef3709c4",
   "metadata": {},
   "source": [
    "### Comando 1,2 e 3 iguais ao notebook no Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cd0b32c-277b-4a50-b528-bec199c21a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>914</td><td>application_1653433423608_0988</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-150-1-85.ec2.internal:20888/proxy/application_1653433423608_0988/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-150-1-22.ec2.internal:8042/node/containerlogs/container_1653433423608_0988_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = f'''\n",
    "select distinct\n",
    "        date_format(t.datageracao, 'y-MM') as year_month,\n",
    "        min(id_transacao) over (partition by t.id_conta, date_format(t.datageracao, 'y-MM'),t.issuerpartition) as id_transacao,\n",
    "        t.issuerpartition as issuer_id,\n",
    "        t.id_conta as account_id\n",
    "    from cleaned_issuing.transacoescorrentes t\n",
    "        left join cleaned_issuing.tipostransacoes tt\n",
    "            on t.issuerpartition = tt.issuerpartition\n",
    "            and t.id_tipotransacao = tt.id_tipotransacao\n",
    "    where\n",
    "        t.datageracao >= '2021-01-01' \n",
    "        and tt.flagcredito = 'false' \n",
    "        and t.id_conta not in (1,2,3,4,5) \n",
    "        and tt.flagextrato = 'true'\n",
    "        \n",
    "    '''\n",
    "spark.sql(query).createOrReplaceTempView('active_account_midas')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aef9b65-70b3-4304-bd8a-94a3c59eb173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = f'''\n",
    "select t.*, tt.descricao\n",
    "from cleaned_issuing.transacoescorrentes t\n",
    "left join cleaned_issuing.tipostransacoes tt  on t.issuerpartition = tt.issuerpartition and t.id_tipotransacao = tt.id_tipotransacao\n",
    " '''\n",
    "spark.sql(query).createOrReplaceTempView('comp_info')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785d3832-5582-4818-a414-64802411e62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = f'''\n",
    "select a.year_month, a.issuer_id, a.account_id, t.id_tipotransacao, t.descricao, a.id_transacao, t.datageracao \n",
    "from active_account_midas a\n",
    "inner join comp_info t on a.issuer_id = t.issuerpartition and a.id_transacao = t.id_transacao\n",
    "'''\n",
    "##spark.sql(query).write.mode(\"overwrite\").saveAsTable(\"lakehouse.bt_midas_activeaccount\", overwrite=True, format='parquet', path='s3://dock-datalake-prd/lakehouse/bt_midas_activeaccount')\n",
    "query = spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95cd05-6efe-401b-91ff-e1768c6e1151",
   "metadata": {},
   "source": [
    "### Como é feito no Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c4f68-2fbf-43b8-a0a0-687617a30034",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = 'databankstatements-prd'\n",
    "host = dbutils.secrets.get(scope, \"host\")\n",
    "port = dbutils.secrets.get(scope, \"port\")\n",
    "database = dbutils.secrets.get(scope, \"database\")\n",
    "username = dbutils.secrets.get(scope, \"username\")\n",
    "password = dbutils.secrets.get(scope, \"password\")\n",
    "output_table=f\"midas.bt_active_account\"\n",
    "\n",
    "query\\\n",
    "    .write.options(truncate='true')\\\n",
    "    .format('jdbc')\\\n",
    "    .mode('overwrite')\\\n",
    "    .options(url=f'jdbc:postgresql://{host}:{port}/{database}?rewriteBatchedStatements=true', dbtable=output_table, user=username, password=password).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c19e64d-fb75-4b11-bb0d-c8a3ea6bf345",
   "metadata": {},
   "source": [
    "### Adaptação que a Camis me ajudou "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eed13849-9a85-4734-a837-3247b2df2a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "could not connect to server: Connection timed out\n",
      "\tIs the server running on host \"data-bank-statements.cw1dtrln74e9.sa-east-1.rds.amazonaws.com\" (10.140.3.82) and accepting\n",
      "\tTCP/IP connections on port 5439?\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/psycopg2/__init__.py\", line 122, in connect\n",
      "    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)\n",
      "psycopg2.OperationalError: could not connect to server: Connection timed out\n",
      "\tIs the server running on host \"data-bank-statements.cw1dtrln74e9.sa-east-1.rds.amazonaws.com\" (10.140.3.82) and accepting\n",
      "\tTCP/IP connections on port 5439?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import psycopg2\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.orc.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "spark.conf.set(\"spark.sql.sources.partitionColumnTypeInference.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "client = boto3.client('secretsmanager',\"us-east-1\")\n",
    "response = client.get_secret_value(\n",
    "    SecretId='Databricks_DatabankstatementsPrdDbPassword'\n",
    ")\n",
    "\n",
    "connectionProperties = {\\\n",
    "                        \"user\":json.loads(response['SecretString'])['username'],\\\n",
    "                        \"password\":json.loads(response['SecretString'])['password'],\\\n",
    "                        \"host\":json.loads(response['SecretString'])['host'],\\\n",
    "                        \"dbname\":json.loads(response['SecretString'])['dbname']\\\n",
    "                       }\n",
    "\n",
    "host = json.loads(response['SecretString'])['host']\n",
    "port = 5439\n",
    "database = json.loads(response['SecretString'])['dbname']\n",
    "username = json.loads(response['SecretString'])['username']\n",
    "password = json.loads(response['SecretString'])['password']\n",
    "materialized_view = 'midas.bt_active_account'\n",
    "\n",
    "con=psycopg2.connect(\n",
    "  dbname= database,\n",
    "  host=host, \n",
    "  port=port,\n",
    "  user=username,\n",
    "  password=password)\n",
    "\n",
    "query\\\n",
    "    .write.options(truncate='true')\\\n",
    "    .format('jdbc')\\\n",
    "    .mode('overwrite')\\\n",
    "    .options(url=f'jdbc:postgresql://{host}:{port}/{database}?rewriteBatchedStatements=true', dbtable=output_table, user=username, password=password).save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcccae8-6194-43c1-8e8c-d9f346f11ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
